---
title: "Gestion_GRA_Clauss"
output: html_document
date: "`r Sys.Date()`"
editor_options: 
  markdown: 
    wrap: 72
---

# 1. Introduction

## Packages

We import the necessary libraries to perform the first analysis.In the
provided R code snippet, the library() function is employed to load and
make accessible various R packages within your current R session. Let's
elucidate the purpose of each of these packages:

library(tidyverse): The tidyverse package constitutes a collection of R
packages meticulously designed to work seamlessly in unison for data
manipulation, visualization, and analysis. It encompasses packages like
ggplot2 for data visualization, dplyr for data manipulation, tidyr for
data tidying, and more. Loading tidyverse avails all these packages for
utilization during your R session, streamlining your data analysis
workflow.

library(quantmod): The quantmod package primarily caters to quantitative
financial modeling and analysis. It furnishes an array of functions and
tools for handling financial data, such as downloading and managing
stock price data, conducting technical analysis, and modeling financial
time series.

library(DataExplorer): The DataExplorer package serves the purpose of
preliminary data exploration and analysis. It offers functions to
generate summary statistics, visualize missing data, plot variable
distributions, and more. It can aid in obtaining a quick overview of
your data before delving into more extensive analyses.

library(corrplot): The corrplot package specializes in visualizing
correlation matrices. It provides functions for generating visually
appealing and informative correlation plots, which prove valuable for
comprehending relationships between variables within your data.

library(scales): The scales package provides an assortment of scales and
formatting functions tailored for R graphics. It is often employed in
conjunction with other plotting packages, such as ggplot2, to customize
the appearance of plots, including axes, labels, and color scales.

```{r Packages, include=FALSE}
library(tidyverse)
library(quantmod)
library(DataExplorer)
library(corrplot)
library(scales)
```

## Investment universe

In our portfolio management project, we have carefully chosen a set of
20 stocks from a larger universe of investments. This selection
represents the most significant companies in the US equity market,
primarily based on their market capitalization. By analyzing these
stocks, our objective is to gain valuable insights into the strategies
that can be employed for managing investments in the US equity market.

Our choice of stocks is guided by several key considerations:

Investment Universe: Our selection is drawn from a universe of stocks
that encompasses 20 influential and well-established companies within
the US equity market. These companies are known for their substantial
market presence and are recognized leaders in their respective
industries.

Market Capitalization Focus: We have chosen stocks with an emphasis on
market capitalization. Larger market capitalization often indicates
stability and lower volatility, making these stocks appealing for
various investment strategies.

US Equity Market Perspective: Our project primarily targets insights
related to the US equity market. The chosen stocks provide a
representative snapshot of the investment landscape in one of the
world's largest and most influential equity markets.

Diversification: To create a well-rounded portfolio, we have ensured
diversification across different sectors and industries. Diversification
is essential in portfolio management as it spreads risk and mitigates
overexposure to a single sector.

Market Visibility: The selected companies are not only well-regarded in
the financial sector but are also widely recognized in popular culture.
Their performance is closely monitored, and they often hold significant
sway over market indices.

Historical Data and Research: These stocks have a rich history of
performance data and have been the subject of extensive research. Access
to their historical data is abundant, making them suitable for rigorous
portfolio management analysis and research.

The provided R code defines the stock tickers for these carefully chosen
companies, allowing you to access financial data. This data can then be
employed to conduct a range of analyses, such as return calculations,
risk assessments, and portfolio optimization, to test and implement
various investment strategies on the US equity market.

```{r}
# Define the stock tickers you want to fetch data for
stock_tickers <- c("AAPL", "MSFT", "GOOGL", "AMZN", "TSLA", "NVDA", "PYPL", "CMCSA", "ADBE", "ASML",
                   "CSCO", "PEP", "AVGO", "TMUS", "INTC", "TXN", "AMGN", "NFLX", "SBUX", "AMAT")
```

We source our data from Yahoo Finance, a highly reputable platform
renowned for its financial data. Our analysis covers a comprehensive
three-and-a-half-year period, allowing us to evaluate the performance of
our selected companies both before and after the onset of the COVID-19
pandemic.

We source our data from Yahoo Finance, a highly reputable platform
renowned for its financial data. Our analysis covers a comprehensive
three-and-a-half-year period, allowing us to evaluate the performance of
our selected companies both before and after the onset of the COVID-19
pandemic.The start date is from 01/01/2019 to 31/08/2023.

```{r Yahoo Finance extraction data, echo=FALSE}
# Fetch historical stock data from Yahoo Finance
start_date <- "2019-01-01"  # Replace with your desired start date
end_date <- "2023-08-31"    # Replace with your desired end date
stock_data <- getSymbols(stock_tickers, from = start_date, to = end_date, auto.assign = TRUE)
```


## Data cleaning

We perform some formatting to clean the data, an important step in
performing data analysis.

Extracting Adjusted Closing Prices: lapply(stock_tickers,
function(ticker) { Ad(get(ticker)) }): This code uses the lapply
function to retrieve the adjusted closing prices (often used for return
calculations) for a list of stock tickers. It iterates through each
ticker in the stock_tickers vector and fetches the adjusted closing
prices using the Ad() function from the quantmod package.

Combining Stock Returns: do.call(cbind, stock_returns): The code
combines the extracted adjusted closing prices into a single data frame
by calling cbind (column-bind) on the list of stock returns obtained in
the previous step. This creates a data frame where each column
represents the adjusted closing prices of a specific stock.

Renaming Columns: colnames(fin_prices) \<- stock_tickers: The code
assigns the stock tickers as column names to the combined data frame.
This step ensures that each column is labeled with the corresponding
stock ticker, making it easier to identify each stock's data.

Handling Missing Data: na.omit(fin_prices): This line of code removes
rows with missing values (usually represented as NA) from the combined
data frame, fin_prices. Removing missing data is a common preprocessing
step to ensure the quality and consistency of the dataset for further
analysis.


```{r Stock return calculation, echo=FALSE}
# Extract adjusted closing prices (which are typically used for returns)
stock_returns <- lapply(stock_tickers, function(ticker) {
  to.monthly(Ad(get(ticker)), indexAt = 'lastof', OHLC = FALSE)
})

# Combine stock returns into a single data frame
fin_prices <- do.call(cbind, stock_returns)

# Rename columns to match your analysis
colnames(fin_prices) <- stock_tickers

# Eliminated missing data in time series
fin_return <- na.omit(fin_prices)  # Remove rows with missing values
```

We perform the arithmetic return method to compute returns for this data
set. It is simple, intuitive and widely accessible. Arithmetic returns
are particularly useful for performance measurement, providing a clear
view of actual gains and losses. It is well-suited for short-term
analysis and offer transparency.

```{r Return calculation, include=FALSE}
arith_fin_returns = diff(fin_return)/lag(fin_return)
head(arith_fin_returns, n=3)
arith_fin_returns <- arith_fin_returns[-1, ]
```

In order to ensure that the data is properly imported, with non missing
values and proper data cleaning procedure, we can implement this line of
code in order to learn more about data.

```{r Plot of data structure, include=FALSE}
plot_intro(arith_fin_returns)
```

From the figure below, Tesla's stock (TICKER: TSLA) witnessed an
unprecedented surge in attraction, making it a notable standout within
our investment universe.

Tesla's performance distinctly outpaced the broader investment universe.
The surge in retail trading interest for the electric car company can
explain to some extent this massive performance, especially in the wake
of the post-COVID market, where intensive monetary policies prompted
significant investments from both corporate and retail investors seeking
higher yields.

The code allowed us to plot returns into a time series, providing a
clear visualization of stock performance during the specified period.

```{r}
# Calculate Cumulative Returns
cumulative_returns <- cumprod(1 + arith_fin_returns)

# Plot Cumulative Return Performance
library(ggplot2)
autoplot(cumulative_returns, facets = NULL) +
  ggtitle("Cumulative Return Performance of the investment universe") +
  ylab("Cumulative Return") +
  xlab("Year")

```

We can define the minimum volatility portfolio as a portfolio of assets
with the lowest possible risk for an investor and is located on the
far-left side of the efficient frontier. Note that the minimum
volatility portfolio is also called the minimum variance portfolio or
more precisely the global minimum volatility portfolio (to distinguish
it from other optimal portfolios obtained for higher risk levels).

To implement to global minimum variance portfolio, we need to compute
the covariance matrix as an input parameter for the Markowitz
optimization model.

```{r}
cov_matrix <- cov(arith_fin_returns)

```

Correlation plays an important role in a Markowitz allocation framework,
where diversification can be achieved when allocating capital to poorly
or negatively correlated assets.

The concept of diversification seeks to enhance returns while minimizing
risk by investing in a variety of assets that will react differently to
the same event (s). For example, whenever there is unfavorable news
about a certain event, i.e. 2008 subprime mortgage crisis, the stock
market typically declines dramatically. Simultaneously, the same news
has generally benefited the price of specific assets, such as gold. As a
result, portfolio diversification should include not just diverse stocks
inside and outside of the same industry, but also diverse asset classes,
such as bonds and commodities. The diversification effect is a term that
relates to the link between portfolio correlations and diversification.
When there is an imperfect correlation between assets (positive or
negative), the diversification effect occurs. It is a critical and
successful risk mitigation method since risk mitigation may be
accomplished without sacrificing profits. As a result, any prudent
investor who is 'risk cautious' will diversify to a certain extent.

Here, we observe opportunities for diversification within our investment
universe. For instance, we notice that stocks often categorized as
defensive plays can serve as effective diversification instruments when
combined with more aggressive stocks. In this context, consider Amgen
Inc. (TICKER: AMGN) in the healthcare industry, which exhibits a neutral
correlation when compared to a stock like Tesla (TICKER: TSLA). Similar
logic applies to a stock such as PepsiCo Inc. (TICKER: PEP) in relation
to Tesla. It's worth noting that within our investment universe, we do
not find negative correlations, as all the assets exhibit some degree of
correlation to one another.

```{r}
corrplot(cor(arith_fin_returns), type='lower', 
         method = "shade", tl.col = 'black', cl.pos = "r", tl.cex = 1)
```

To further complement the analysis of the data gathered, we can run a
statistical analysis of the first moments of the distribution to assess
the performance of each stock and understand their behavior during the
timeframe observed. NB: Figures obtained from this statistical analysis
are given on a daily return basis.

```{r}
summary(arith_fin_returns)
```

In portfolio management, statistical analysis can help understand
financial patterns in data.

First, density plots, also known as probability density function plots,
serve the purpose of visualizing data distributions. They offer insights
into the shape, spread, and central tendencies of data, making them
crucial for assessing the nature of financial asset returns.

Second, QQ plots, or quantile-quantile plots, are utilized to evaluate
whether a dataset adheres to a specific theoretical distribution, such
as the normal distribution. These plots help portfolio managers identify
departures from expected distributions, facilitating informed investment
decisions. Together, these techniques aid in the exploration and
assessment of financial data, enabling portfolio managers to make more
informed and data-driven choices in their investment strategies.

We can run some more advanced statistical analysis to understand the
asset return behavior of the equities selected in this analysis.

```{r}
plot_density(arith_fin_returns)
plot_qq(arith_fin_returns)

```

# 2. Modelling part

Modern Portfolio Theory (MPT) is founded on several market and investor
assumptions. Several of these assumptions are stated explicitly, while
others are implied. Markowitz\'s contributions to (MPT) in portfolio
selection are based on the following basic assumptions:Â 

-   Investors are rational (they seek to maximize returns while
    minimizing risk, or minimize risk while maximize return).

-   Investors will accept increased risk only if compensated with higher
    expected returns.

-   Investors receive all relevant information regarding their
    investment decision.

-   Investors can borrow or lend an unlimited amount of capital at a
    risk-free rate of interest.

## Unbiased Global Minimum Variance (GMV) portfolio

In the context of time series data analysis, one of the crucial
applications is the division of the data set into a training set and a
testing set based on temporal order. This approach is particularly
important when dealing with data that evolves over time, such as stock
prices, weather observations, or economic indicators.

The training set typically comprises historical data, while the testing
set contains more recent observations. This temporal separation allows
for the evaluation of predictive models and forecasting techniques. By
using historical data to train the model and then assessing its
performance on more recent data, analysts can gauge the model's ability
to make accurate predictions and anticipate future trends. Time series
data applications are essential in fields like finance, where
understanding and forecasting trends over time is important.

For the purpose of modelling, we will shrink the data set into two
different sub-samples. We can define the following parameters:

```{r Subsampling of data, include=FALSE}
num_rows_subsample1 <- 20
total_rows <- nrow(arith_fin_returns)
```

1.  The first sub-sample will cover the first 20 trading month covered
    in the data set.

```{r Subsample 1 data structure, echo=FALSE}
subsample1 <- arith_fin_returns[1:num_rows_subsample1, ]
plot_intro(subsample1)
```

2.  The second sub-sample will cover the rest of the data set, covering
    the equivalent of 36 trading month.

```{r Subsample 2 data structure, echo=FALSE}
subsample2 <- arith_fin_returns[(num_rows_subsample1 + 1):total_rows, ]
plot_intro(subsample2)

```

We implement the parameters of the model in order to compute the GMV:

```{r Parameters calculations, echo=FALSE}

#Definition of parameters
n <- ncol(subsample1)
T <- nrow(subsample1)
e <- rep(1, n)
perio <- 12

#Compute Sigma (unbiased covariance matrix)
#We proceed with a regularization procedure: In order to compute an inverse, we can add a small positive value to the diagonal of the matrix to make it invertible. This technique is known as "Tikhonov regularization" or "ridge regression".

Sigma <- cov(subsample1) * (T - 1) / (T - n - 2) * perio
lambda <- 1e-4  # Small regularization parameter
Sigma_reg <- Sigma + diag(lambda, ncol(Sigma))
C <- t(e) %*% solve(Sigma_reg) %*% e

#Anticipated volatility can be computed
sigmag <- sqrt(1 / C)
```

After computing the parameters required to implement the unbiased GMV
portfolio, we can compute omega, representing the weightings of the
portfolio as follows.

```{r Omega GMV parameter, include=FALSE}
omega_gmv <- 1 / as.numeric(C) * solve(Sigma_reg) %*% e
```

We can see in this instance that the portfolio is mixed, with some long
and short positions. The capital is fully invested, with some important
exposure in the long part of the portfolio on stocks like Adobe (TICKER:
ADBE) with 94.61%, followed by PepsiCo (TICKER: PEP) at 73.15%. and
T-Mobile (TICKER: TMUS) at 55.69%.

On the short part of the portfolio, we can see an important short on
three stocks that can stand out: Microsoft (TICKER: MSFT) -49.83%, ASML
corporation (TICKER: ASML) at -43.22% and Paypal (TICKER: PYPL) at
-40.79%.

```{r GMV plot , echo=FALSE}
barplot(as.numeric(omega_gmv), col = 'black', ylim = c(-0.5, 1))
```


## Implementation of Principal Component Analysis (PCA) 

PCA can be used for a variety of tasks, including dimensionality
reduction, data visualization, and feature extraction. In the context of
portfolio management, PCA can be used to:

-   Identify the underlying factors that drive the returns of the stocks
    in the portfolio.

-   Construct more diversified portfolios by selecting stocks with low
    correlations with each other.

-   Reduce the dimensionality of the portfolio by selecting the most
    important principal components.

The PCA results can be interpreted as follows:

-   The first principal component can be thought of as a market factor.
    This factor represents the overall movement of the stock market. In
    other terms, the first factor explains half of the variance of the
    portfolio overall and can be assimilated as a proxy to the
    sensitivity to market fluctuations.

-   The second and third principal components can be thought of as style
    factors. These factors represent the different styles of investing,
    such as value investing and growth investing. From the type of
    stocks retained, we have shortlisted an important number of growth
    stocks in the technology sector, which could explain part of the
    overall variance of the portfolio.

-   The remaining principal components can be thought of as
    idiosyncratic factors. These factors are specific to the individual
    stocks in the portfolio.

The below plot represents the portfolio allocation using the PCA method.

```{r PCA three factor model, echo=FALSE}
#PCA one factor

# valp <- eigen(Sigma_reg)$values
# vecp <- eigen(Sigma_reg)$vectors
# vp1 <- vecp[, 1]
# lambda1 <- valp[1]
# varepsilon1 <- diag(Sigma_reg) - vp1 ^ 2 * lambda1
# Sigma_epsilon1 <- diag(varepsilon1, n, n)
# Sigma1 <- (lambda1 * vp1 %*% t(vp1) + Sigma_epsilon1)
# C1 <- t(e) %*% solve(Sigma1) %*% e
# sigmag1 <- sqrt(1 / C1)
# omega1 <- 1 / as.numeric(C1) * solve(Sigma1) %*% e

#plotting the weights for the one factor model
# barplot(as.numeric(omega1), col = 'black')


#PCA for three factor 
valp <- eigen(Sigma_reg)$values
vecp <- eigen(Sigma_reg)$vectors

vp3 <- cbind(vecp[, 1], vecp[, 2], vecp[, 3])
lambda3 <- diag(c(valp[1], valp[2], valp[3]), 3, 3)
varepsilon3 <- diag(Sigma_reg) - vp3 ^ 2 %*% diag(lambda3)
Sigma_epsilon3 <- diag(as.numeric(varepsilon3), n, n)
Sigma3 <- (vp3 %*% lambda3 %*% t(vp3) + Sigma_epsilon3)
C3 <- t(e) %*% solve(Sigma3) %*% e
sigmag3 <- sqrt(1 / C3)
omega3 <- 1 / as.numeric(C3) * solve(Sigma3) %*% e

#plotting the weights for the three factor model
barplot(as.numeric(omega3), col = 'black')
```
